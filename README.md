# PW22PK03

## Team Members:
- Ishan Rai (PES1201800818)
- Varun Kadam (PES1201800790)
- Harsha Adimulam (PES1201800231)
- K. Sreesh Reddy (PES1201801580)

## Mentor: 
Prof. Preet Kanwal, Associate Professor, Dept of CSE, PES University

## Published Paper Link:
The paper was accepted and presented at the 3rd IEEE International Conference of Emerging Technologies ([IEEE INCET](http://www.incet.org/#)) and included into IEEE Xplore: https://ieeexplore.ieee.org/document/9824614 and is Scopus Indexed.


## Problem Statement
ederated Learning is a novel technology that aims to facilitate the training of ML models using the collaborative power of all its users by allowing them to download the models and train them locally. This enables the clients to maintain privacy while contributing to the global training of the model. Due to the increasingly complex nature of modern ML models, a lot of data is generated that leads to heavy loads on the network hosting the system. In this paper, we discuss a method to significantly reduce communication costs by reconstructing the generated updates using sparsification to eliminate the lower bound values, k means clustering to reduce the entire dataset to a discrete set of values and then encoding them. The proposed compression methodologies will be integrated with LEAF which is a pre-existing framework used to simulate a real-world federated setting with multiple clients and an aggregating server. 
